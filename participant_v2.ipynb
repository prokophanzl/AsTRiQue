{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a383e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============\n",
    "# CONFIG\n",
    "# ==============\n",
    "\n",
    "import astrique_module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from playsound import playsound\n",
    "\n",
    "PREDICTOR1 = 'voicing'                # first predictor column name\n",
    "PREDICTOR2 = 'duration'               # second predictor column name\n",
    "FILENAME_COL = 'filename'             # filename column name\n",
    "LABEL_MAPPING = {'s': 0, 'z': 1}      # binary output label mapping\n",
    "\n",
    "TARGET = 'answer_batch'               # target column name\n",
    "DATA_PATH = 'data/data.csv'           # sound info data file path\n",
    "PROCESSED_PATH = 'data_processed.csv' # processed data file path; leave blank to disable\n",
    "AUDIO_FOLDER = 'data/audio'           # audio file directory\n",
    "\n",
    "INIT_RANDOM_SAMPLES = 10              # initial random samples to collect\n",
    "MIN_ITERATIONS = 30                   # minimum number of iterations\n",
    "CLEANSER_FREQUENCY = 0                # insert a high-certainty sample every nth iteration to prevent participant fatigue (irrelevant for virtual agents); 0 to disable\n",
    "MODEL_CERTAINTY_CUTOFF = 0.95         # stopping certainty threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============\n",
    "# PARTICIPANT FUNCTIONS\n",
    "# ==============\n",
    "\n",
    "def query_participant_classification(filename, wait_for_enter = False):\n",
    "    \"\"\"\n",
    "    Plays audio and waits for valid human response ('s' or 'z'). Replays on invalid input.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(AUDIO_FOLDER, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Missing file: {filepath}. Skippin g.\")\n",
    "        return None\n",
    "\n",
    "    while True:\n",
    "        if wait_for_enter:\n",
    "            input(f\"\\nReady to hear the sound '{filename}'? Press Enter to play...\")\n",
    "\n",
    "        try:\n",
    "            playsound(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"Error playing sound: {e}\")\n",
    "            return None\n",
    "\n",
    "        response = input(\"Enter your response ('s' or 'z'): \").strip().lower()\n",
    "        if response in LABEL_MAPPING:\n",
    "            return LABEL_MAPPING[response]\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter 's' or 'z'. Replaying sound...\")\n",
    "            wait_for_enter = False  # skip Enter prompt on replays\n",
    "\n",
    "def evaluate_model(stimuli, filename_col):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions on the unanswered data by comparing them to real labels\n",
    "    obtained via query_participant_classification().\n",
    "    \"\"\"\n",
    "    # get unanswered data\n",
    "    unanswered = stimuli[stimuli['participant_classification'].isna()].copy()\n",
    "    \n",
    "    if unanswered.empty:\n",
    "        print(\"No unanswered data to evaluate.\")\n",
    "        return\n",
    "\n",
    "    # query the actual class for evaluation\n",
    "    true_labels = []\n",
    "    predicted_labels = unanswered['predicted_class'].tolist()\n",
    "\n",
    "    print(\"Evaluating model predictions on unanswered data...\")\n",
    "\n",
    "    for filename in unanswered[filename_col]:\n",
    "        # print which sound is being evaluated out of how many - count filenames from 1\n",
    "        print(f\"Evaluating sound {unanswered[filename_col].tolist().index(filename) + 1} out of {len(unanswered[filename_col])}\")\n",
    "        true_label = int(query_participant_classification(filename))\n",
    "        true_labels.append(true_label)\n",
    "\n",
    "    # calculate metrics\n",
    "    acc = accuracy_score(true_labels, predicted_labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "    print(\"\\n=== Evaluation on Unanswered Data ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "def plot_results(stimuli, model):\n",
    "    \"\"\"Visualize results with decision boundary and improved legend/color bar\"\"\"\n",
    "    \n",
    "    # split answered and unanswered data\n",
    "    answered_data = stimuli[stimuli['participant_classification'].notna()]\n",
    "    unanswered_data = stimuli[stimuli['participant_classification'].isna()]\n",
    "\n",
    "    plt.figure(figsize=(10, 6), dpi=300)\n",
    "    \n",
    "    # convert answers to numeric if necessary\n",
    "    if answered_data['participant_classification'].dtype == 'object':\n",
    "        answered_data = answered_data.copy()\n",
    "        answered_data['participant_classification'] = answered_data['participant_classification'].astype(int)\n",
    "        \n",
    "    # plot answered points, split by class\n",
    "    for label_char, label_num in LABEL_MAPPING.items():\n",
    "        subset = answered_data[answered_data['participant_classification'] == label_num]\n",
    "        if not subset.empty:\n",
    "            plt.scatter(\n",
    "                subset[PREDICTOR1],\n",
    "                subset[PREDICTOR2],\n",
    "                c='blue' if label_num == 0 else 'red',\n",
    "                label=f\"answered ({label_char})\",\n",
    "                edgecolors='k'\n",
    "            )\n",
    "\n",
    "    # plot unanswered points\n",
    "    if not unanswered_data.empty:\n",
    "        plt.scatter(\n",
    "            unanswered_data[PREDICTOR1], \n",
    "            unanswered_data[PREDICTOR2],\n",
    "            c='gray',\n",
    "            alpha=0.5,\n",
    "            label='unanswered'\n",
    "        )\n",
    "    \n",
    "    # decision boundary grid\n",
    "    x_min, x_max = stimuli[PREDICTOR1].min() - 1, stimuli[PREDICTOR1].max() + 1\n",
    "    y_min, y_max = stimuli[PREDICTOR2].min() - 1, stimuli[PREDICTOR2].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    grid_points = pd.DataFrame(\n",
    "        np.c_[xx.ravel(), yy.ravel()],\n",
    "        columns=[PREDICTOR1, PREDICTOR2]\n",
    "    )\n",
    "    \n",
    "    Z = model.predict_proba(grid_points)[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # show background decision gradient\n",
    "    contour = plt.contourf(xx, yy, Z, alpha=0.3, levels=20, cmap='coolwarm')\n",
    "    \n",
    "    # custom color bar with labels s and z\n",
    "    cbar = plt.colorbar(contour, ticks=[0, 1])\n",
    "    rev_label_map = {v: k for k, v in LABEL_MAPPING.items()}\n",
    "    cbar.ax.set_yticklabels([rev_label_map[0], rev_label_map[1]])\n",
    "    cbar.set_label('predicted answer')\n",
    "    \n",
    "    plt.xlabel(PREDICTOR1)\n",
    "    plt.ylabel(PREDICTOR2)\n",
    "    plt.title(f'Virtual Agent Results (participant: {PARTICIPANT_TO_MODEL})')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============\n",
    "# MAIN EXECUTION\n",
    "# ==============\n",
    "\n",
    "# create stimuli dataframe\n",
    "stimuli = pd.read_csv(DATA_PATH)\n",
    "\n",
    "astrique_module.initialize_dataframe(stimuli)\n",
    "\n",
    "# initial random sampling with class balance\n",
    "collected_classes = set()\n",
    "\n",
    "iteration = 1\n",
    "while iteration <= INIT_RANDOM_SAMPLES or len(collected_classes) < 2:\n",
    "    print(f\"Iteration {iteration}: Random sampling\")\n",
    "\n",
    "    # select a random stimulus where real class is unknown\n",
    "    sample = stimuli[stimuli['participant_classification'].isna()].sample(1)\n",
    "\n",
    "    # get classification, querying filename\n",
    "    classification = int(query_participant_classification(sample[FILENAME_COL].values[0]))\n",
    "    \n",
    "    collected_classes.add(classification)\n",
    "\n",
    "    # update row in dataframe\n",
    "    idx = stimuli[FILENAME_COL] == sample[FILENAME_COL].values[0]\n",
    "    stimuli.loc[idx, 'classification_order'] = iteration\n",
    "    stimuli.loc[idx, 'classification_type'] = 'random'\n",
    "    stimuli.loc[idx, 'participant_classification'] = classification\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "# train initial model\n",
    "model = astrique_module.train_model(stimuli)\n",
    "\n",
    "# active learning phase\n",
    "active_learning_iteration = 1\n",
    "\n",
    "while True:\n",
    "    # retrain model to get up-to-date predictions on remaining unlabeled samples\n",
    "    model = astrique_module.train_model(stimuli)\n",
    "\n",
    "    # get updated unanswered subset\n",
    "    unanswered = stimuli[stimuli['participant_classification'].isna()]\n",
    "    \n",
    "    # check stopping condition\n",
    "    below_cutoff = unanswered['prediction_certainty'] < MODEL_CERTAINTY_CUTOFF\n",
    "    if below_cutoff.sum() == 0 and active_learning_iteration >= MIN_ITERATIONS:\n",
    "        print(\"Stopping active learning: all predictions above certainty threshold \"\n",
    "              f\"({MODEL_CERTAINTY_CUTOFF}) and minimum iterations met ({MIN_ITERATIONS}).\")\n",
    "        break\n",
    "\n",
    "    # select next sample using uncertainty sampling (with optional cleanser)\n",
    "    sample, sample_type = astrique_module.get_sample(unanswered, iteration, active_learning_iteration)\n",
    "\n",
    "    # query real classification\n",
    "    classification = int(query_participant_classification(sample[FILENAME_COL].values[0]))\n",
    "\n",
    "    # update row in dataframe\n",
    "    idx = stimuli[FILENAME_COL] == sample[FILENAME_COL].values[0]\n",
    "    stimuli.loc[idx, 'classification_order'] = iteration\n",
    "    stimuli.loc[idx, 'classification_type'] = sample_type\n",
    "    stimuli.loc[idx, 'participant_classification'] = classification\n",
    "\n",
    "    iteration += 1\n",
    "    active_learning_iteration += 1\n",
    "\n",
    "# plot the results\n",
    "plot_results(stimuli, model)\n",
    "\n",
    "# save dataframe\n",
    "if PROCESSED_PATH:\n",
    "    astrique_module.export_data(stimuli, PROCESSED_PATH)\n",
    "else:\n",
    "    print(\"Processed data not saved - PROCESSED_PATH is empty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1ac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "evaluate_model(stimuli, FILENAME_COL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astrique",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
